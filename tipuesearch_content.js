var tipuesearch = {"pages":[{"title":"高效去重算法Bloomfilter","text":"基于redis实现Bloomfilter #!usr/bin/env python # -*- coding:utf-8 -*- \"\"\" @author: nico @file: bloomfilter_redis.py @time: 2018/08/31 \"\"\" import math import mmh3 class BloomFilter : def __init__ ( self , n , f , server , block_num = 1 , key_prefix = 'BLOOMFILTER' ): \"\"\" m: number of bit as least to be assign k: number of hash as least need :param n: number of items is going to add :param f: expected false positive probability :param server: the redis client instance :param block_num: number of redis block, one of block maxsize 512m , 2**32 :param key_prefix: the block key prefix \"\"\" if not ( 0 < f < 1 ): raise ValueError ( \"f must be between 0 and 1.\" ) if not n > 0 : raise ValueError ( \"n must be > 0\" ) self . n = n self . f = f self . k = math . ceil ( math . log ( 1.0 / f , 2 )) self . m = 1 << 31 # 2**32 self . server = server self . key_prefix = key_prefix self . block_num = block_num def __contains__ ( self , item ): item = str ( item ) key = self . __get_block_route_key ( item ) res = True for seed in range ( self . k ): offset = mmh3 . hash ( item , seed , signed = False ) res = res & self . server . getbit ( key , offset % self . m ) return True if res else False def add ( self , item ): item = str ( item ) self . key = self . __get_block_route_key ( item ) for seed in range ( self . k ): offset = mmh3 . hash ( item , seed , signed = False ) self . server . setbit ( self . key , offset % self . m , 1 ) def __get_block_route_key ( self , hashable ): return self . key_prefix + str ( sum ( map ( ord , hashable )) % self . block_num ) if __name__ == '__main__' : from redis import StrictRedis client = StrictRedis () bf = BloomFilter ( 100000000 , 0.0001 , client ) test_str = [ \"python\" , \"c\" , \"c++\" , \"ruby\" ] for el in test_str : bf . add ( el ) print ( list ( map ( lambda _ : _ in bf , [ 'lua' , 'python' , 'go' , 'c' ]))) # [False, True, False, True] 基于内存实现Bloomfilter #!usr/bin/env python # -*- coding:utf-8 -*- \"\"\" @author: nico @file: bloomfilter_memory.py @time: 2018/08/31 \"\"\" import math import mmh3 import bitarray class BloomFilter : def __init__ ( self , n , f , block_num = 1 ): \"\"\" m: number of bit as least to be assign k: number of hash as least need :param n: number of items is going to add :param f: expected false positive probability :param block_num: number of bitarray obj, one block maxsize 16Gb on 32 bit systems \"\"\" if not ( 0 < f < 1 ): raise ValueError ( \"f must be between 0 and 1.\" ) if not n > 0 : raise ValueError ( \"n must be > 0\" ) self . n = n self . f = f self . k = math . ceil ( math . log ( 1.0 / f , 2 )) self . m = math . ceil ( - self . k / math . log ( 1 - math . exp ( math . log ( f ) / self . k )) * n ) self . block_num = block_num self . store = { i : bitarray . bitarray ( self . m , endian = 'little' ) for i in range ( self . block_num )} for el in self . store . values (): el . setall ( False ) def __contains__ ( self , item ): item = str ( item ) flag = True for seed in range ( self . k ): offset = mmh3 . hash ( item , seed , signed = False ) flag = flag & self . store [ offset % self . block_num ][ offset % self . m ] return True if flag else False def add ( self , item ): item = str ( item ) for seed in range ( self . k ): offset = mmh3 . hash ( item , seed , signed = False ) self . store [ offset % self . block_num ][ offset % self . m ] = 1 if __name__ == \"__main__\" : bf = BloomFilter ( 100000000 , 0.0001 , block_num = 2 ) test_str = [ \"python\" , \"c\" , \"c++\" , \"ruby\" , 1 , 2 , 3 ] for el in test_str : bf . add ( el ) print ( list ( map ( lambda _ : _ in bf , [ 'lua' , 'python' , 'go' , 'c' , 4 , 2 , 1 ]))) # [False, True, False, True, False, True, True]","tags":"Algorithms","url":"/bloomfilter","loc":"/bloomfilter"},{"title":"ScrapyRedis集成Bloomfilter","text":"新建 dupfilters package #!usr/bin/env python #-*- coding:utf-8 -*- \"\"\" @author: nico @file: __init__.py @time: 2018/08/31 \"\"\" import os import logging from scrapy.dupefilters import BaseDupeFilter , RFPDupeFilter from scrapy.utils.request import request_fingerprint from scrapy.utils.job import job_dir from scrapy_redis.connection import get_redis_from_settings from robot.dupefilters import bloomfilter_memory , bloomfilter_redis logger = logging . getLogger ( 'dupefilters' ) class BloomFilterRedis ( BaseDupeFilter ): def __init__ ( self , n = 100000000 , f = 0.0001 , server = None , block_nums = 1 , key_prefix = 'BLOOMFILTER' , * args , ** kwargs ): self . bf = bloomfilter_redis . BloomFilter ( n , f , server , block_nums , key_prefix ) self . debug = kwargs . get ( 'debug' , True ) self . logdupes = True @classmethod def from_settings ( cls , settings ): server = get_redis_from_settings ( settings ) key_prefix = settings . get ( \"BLOOMFILTER_REDIS_KEY_PREFIX\" , \"BLOOMFILTER\" ) capacity = settings . get ( \"BLOOMFILTER_REDIS_CAPACITY\" , 100000000 ) error_rate = settings . get ( \"BLOOMFILTER_REDIS_FALSE_POSITIVE_PROBABILITY\" , 0.0001 ) block_nums = settings . get ( \"BLOOMFILTER_REDIS_BLOCK_NUMS\" , 1 ) debug = settings . getbool ( 'DUPEFILTER_DEBUG' , False ) return cls ( capacity , error_rate , server , block_nums , key_prefix , debug = debug ) @classmethod def from_spider ( cls , spider ): settings = spider . settings server = get_redis_from_settings ( settings ) key_prefix = settings . get ( \"BLOOMFILTER_REDIS_KEY_PREFIX\" , \"BLOOMFILTER\" ) capacity = settings . get ( \"BLOOMFILTER_REDIS_CAPACITY\" , 100000000 ) error_rate = settings . get ( \"BLOOMFILTER_REDIS_FALSE_POSITIVE_PROBABILITY\" , 0.0001 ) block_nums = settings . get ( \"BLOOMFILTER_REDIS_BLOCK_NUMS\" , 1 ) debug = settings . getbool ( 'DUPEFILTER_DEBUG' , False ) return cls ( capacity , error_rate , server , block_nums , key_prefix , debug = debug ) @classmethod def from_crawler ( cls , crawler ): return cls . from_settings ( crawler . settings ) def request_seen ( self , request ): fp = request_fingerprint ( request ) if fp in self . bf : return True else : self . bf . add ( fp ) return False def open ( self ): pass def close ( self , reason ): if self . bf . key : self . bf . server . delete ( self . bf . key ) def log ( self , request , spider ): if self . debug : msg = \"Filtered duplicate request: %(request)s \" logger . debug ( msg , { 'request' : request }, extra = { 'spider' : spider }) elif self . logdupes : msg = ( \"Filtered duplicate request %(request)s \" \" - no more duplicates will be shown\" \" (see DUPEFILTER_DEBUG to show all duplicates)\" ) logger . debug ( msg , { 'request' : request }, extra = { 'spider' : spider }) self . logdupes = False @classmethod def from_crawler ( cls , crawler ): return cls . from_settings ( crawler . settings ) def request_seen ( self , request ): fp = request_fingerprint ( request ) if fp in self . bf : return True else : self . bf . add ( fp ) if self . file : self . file . write ( fp + os . linesep ) return False def log ( self , request , spider ): if self . debug : msg = \"Filtered duplicate request: %(request)s \" logger . debug ( msg , { 'request' : request }, extra = { 'spider' : spider }) elif self . logdupes : msg = ( \"Filtered duplicate request %(request)s \" \" - no more duplicates will be shown\" \" (see DUPEFILTER_DEBUG to show all duplicates)\" ) logger . debug ( msg , { 'request' : request }, extra = { 'spider' : spider }) self . logdupes = False 重构ScrapyRedis调度器 #!usr/bin/env python # -*- coding:utf-8 -*- \"\"\" @author: nico @file: schedulers.py @time: 2018/09/02 \"\"\" from scrapy.utils.misc import load_object from scrapy_redis import scheduler as redis_scheduler from scrapy.core import scheduler class RedisScheduler ( redis_scheduler . Scheduler ): def open ( self , spider ): self . spider = spider try : self . queue = load_object ( self . queue_cls )( server = self . server , spider = spider , key = self . queue_key % { 'spider' : spider . name }, serializer = self . serializer , ) except TypeError as e : raise ValueError ( \"Failed to instantiate queue class ' %s ': %s \" , self . queue_cls , e ) try : self . df = load_object ( self . dupefilter_cls )( capacity = spider . settings . get ( \"BLOOMFILTER_REDIS_CAPACITY\" , 100000000 ), error_rate = spider . settings . get ( \"BLOOMFILTER_REDIS_FALSE_POSITIVE_PROBABILITY\" , 0.0001 ), server = self . server , key_prefix = self . dupefilter_key % { 'spider' : spider . name }, block_nums = spider . settings . get ( \"BLOOMFILTER_REDIS_BLOCK_NUMS\" , 1 ), debug = spider . settings . getbool ( 'DUPEFILTER_DEBUG' ), ) except TypeError as e : raise ValueError ( \"Failed to instantiate dupefilter class ' %s ': %s \" , self . dupefilter_cls , e ) if self . flush_on_start : self . flush () # notice if there are requests already in the queue to resume the crawl if len ( self . queue ): spider . log ( \"Resuming crawl ( %d requests scheduled)\" % len ( self . queue )) 在 settings.py 中添加类似如下的配置 SCHEDULER = \"robot.schedulers.RedisScheduler\" DUPEFILTER_CLASS = \"robot.dupefilters.BloomFilterRedis\"","tags":"Scrapy","url":"/scrapyredis-bloomfilter","loc":"/scrapyredis-bloomfilter"}]};